{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "from keras import layers\n",
    "import keras as kr\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the sequential model\n",
    "mod = kr.models.Sequential()\n",
    "# First layer has 512 nodes\n",
    "# without the activation function we would only be able to fit linear models to the data\n",
    "mod.add(kr.layers.Dense(512, activation='relu',input_shape=(28*28,)))\n",
    "# output layer has 10 nodes\n",
    "# softmax is our activation function \n",
    "mod.add(kr.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will configure my model for Training\n",
    "# An optimizer is one of the arguments required for compiling a Keras model\n",
    "# A loss function is the other argument required to compile a our model\n",
    "# A metric is a function that is used to judge the performance of your model\n",
    "\n",
    "# The Adam optimizer is an adaptive learning rate optimization algorithm that's been \n",
    "# designed specifically for training deep neural networks\n",
    "# categorical_crossentropy what this does it computes the difference between the actual output \n",
    "# and the output predicted by the model which then gives us the error in the prediction \n",
    "# this is then given as feedback to the optimizer which in turn will give us a better model with each and every iteration\n",
    "\n",
    "mod.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepocessing our data \n",
    "# flattening out our  28 * 28 pixel matrix\n",
    "x_train = x_train.reshape((60000,28*28))\n",
    "# Normalizing\n",
    "x_train = x_train.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test.reshape((10000,28*28))\n",
    "x_test = x_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating one hot vector of the output labels\n",
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 9s 146us/step - loss: 0.2670 - accuracy: 0.9238\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 8s 137us/step - loss: 0.1067 - accuracy: 0.9686\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 8s 135us/step - loss: 0.0696 - accuracy: 0.9793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1c1007a4128>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Traing our model on the dataset\n",
    "# epochs is the number of iteration's over the entire dataset.\n",
    "# this will take in 4 Parameters (training images, training lable's, epochs, batch size)\n",
    "# using random batches of training date to train the model\n",
    "# the error is decreasing on each epoch\n",
    "# the accuarcy is also increasing with each epoch\n",
    "mod.fit(x_train, y_train, epochs=3, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 92us/step\n"
     ]
    }
   ],
   "source": [
    "# getting the test loss and test accuracy to\n",
    "# Evaluate the model on the test data.\n",
    "test_loss, test_acc = mod.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy:  0.9753000140190125\n"
     ]
    }
   ],
   "source": [
    "# testing the accuracy of our model\n",
    "print('testing accuracy: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving our model in the current directory\n",
    "# this will save the architecture of the model\n",
    "# will the weights of the model\n",
    "# also save the training configuartions of the model\n",
    "mod.save('NewModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "new_model = load_model('NewModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# the architecture of our model \n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.06285134, -0.03085325, -0.06691304, ...,  0.03906752,\n",
       "          0.03229397, -0.02983984],\n",
       "        [-0.05866053, -0.0533441 ,  0.0225852 , ...,  0.05322541,\n",
       "          0.00311256,  0.00368869],\n",
       "        [-0.0203704 ,  0.02294016, -0.00439122, ...,  0.06092392,\n",
       "         -0.0078786 ,  0.02686259],\n",
       "        ...,\n",
       "        [-0.03294227, -0.058896  , -0.02089305, ...,  0.06450939,\n",
       "         -0.03162311,  0.04431283],\n",
       "        [-0.01291958, -0.06491824,  0.06776495, ..., -0.03256425,\n",
       "         -0.04912324,  0.02989183],\n",
       "        [-0.00779596,  0.01860482,  0.01903003, ...,  0.00048782,\n",
       "         -0.03810892,  0.00996786]], dtype=float32),\n",
       " array([ 3.92700061e-02,  3.83294411e-02,  2.65012272e-02,  1.14072403e-02,\n",
       "        -3.17055583e-02,  2.23253723e-02, -1.16883675e-02, -9.38248169e-03,\n",
       "         3.52519588e-03, -2.88258102e-02, -3.95714911e-03,  1.19353496e-02,\n",
       "        -1.67804048e-03,  2.05843914e-02, -1.13489116e-02,  8.03465769e-03,\n",
       "         4.28283922e-02,  1.02156000e-02,  4.27274816e-02, -1.20076025e-02,\n",
       "         2.02250667e-02,  4.61633410e-03, -2.17761099e-02,  1.27486708e-02,\n",
       "         3.51929367e-02,  2.04198007e-02, -1.13228811e-02,  2.09423043e-02,\n",
       "         2.42523346e-02,  5.52426605e-03,  2.40008458e-02, -1.23062739e-02,\n",
       "         3.73119563e-02,  1.48292761e-02,  6.33906648e-02,  3.06131709e-02,\n",
       "         2.97088083e-02, -9.91178211e-03,  2.09153146e-02,  2.08295952e-03,\n",
       "         3.65024768e-02,  4.52552997e-02,  1.99018773e-02,  4.27578911e-02,\n",
       "        -5.68492711e-03, -4.92660468e-03,  8.20979197e-03,  7.61678256e-03,\n",
       "         2.75567062e-02,  2.79287212e-02,  1.22922454e-02, -1.18617658e-02,\n",
       "        -9.13155731e-03, -1.11429598e-02, -4.25464697e-02,  3.27139199e-02,\n",
       "         3.83947766e-03,  2.33709216e-02,  6.89629093e-03,  2.12533623e-02,\n",
       "         3.23831551e-02,  3.54317799e-02, -1.32034672e-02,  6.66488428e-03,\n",
       "        -2.77002156e-02,  2.40548328e-02,  1.17326789e-02,  2.30683200e-03,\n",
       "         2.69339662e-02,  3.97946499e-02,  1.06614595e-02, -2.79639848e-02,\n",
       "         7.89548829e-03,  4.92405035e-02,  5.75312553e-03,  4.09992747e-02,\n",
       "         4.94269393e-02,  1.47813922e-02,  2.25039329e-02,  3.99767309e-02,\n",
       "         2.04952937e-02,  2.28101034e-02,  1.81263560e-04, -2.15890612e-02,\n",
       "         5.19827334e-03,  4.40672711e-02,  1.18021015e-02,  8.42148811e-03,\n",
       "         9.51857958e-03,  2.75065172e-02, -8.13790783e-03,  2.74453256e-02,\n",
       "         1.52868126e-02,  3.83794680e-02, -2.38026660e-02,  3.29934992e-02,\n",
       "        -1.28258299e-02, -5.67774381e-03, -5.94366720e-05, -1.29447160e-02,\n",
       "        -1.50367785e-02,  5.22279507e-03,  6.27022907e-02, -8.12500948e-04,\n",
       "         1.54319918e-02, -6.61670556e-03,  3.12839076e-02, -3.08892000e-02,\n",
       "        -8.85987189e-03,  1.41029749e-02, -4.66029951e-03,  5.44005213e-03,\n",
       "        -3.23913321e-02, -9.37031291e-04,  6.00808766e-03, -8.22762493e-04,\n",
       "         2.75741704e-03, -5.50273666e-03, -3.00517567e-02, -1.72624290e-02,\n",
       "         9.81907826e-04, -7.88854063e-03,  4.17596214e-02,  9.00920108e-03,\n",
       "         4.18297015e-02,  7.03361956e-03, -6.37871623e-02, -1.84449088e-02,\n",
       "        -3.23760621e-02,  1.82855297e-02, -2.79951468e-02, -9.27198399e-03,\n",
       "         3.30864079e-02, -5.49392169e-03, -2.64890082e-02,  2.40707435e-02,\n",
       "        -4.54159090e-05, -2.64489111e-02,  6.07987829e-02, -8.33794009e-03,\n",
       "         1.49091640e-02, -3.70265450e-03,  3.17647271e-02, -1.25641748e-03,\n",
       "        -5.41854091e-03,  2.26570200e-02, -1.75272692e-02,  2.99291387e-02,\n",
       "         1.46794273e-02, -2.16168799e-02,  8.99624825e-03, -7.92326313e-03,\n",
       "         1.74179003e-02,  1.57101974e-02,  3.45041906e-03, -2.21195593e-02,\n",
       "         5.76886581e-03, -2.97088586e-02,  4.69936151e-03,  1.05625838e-02,\n",
       "         2.96952501e-02,  3.68277244e-02,  1.32736694e-02,  1.73378605e-02,\n",
       "         2.43204255e-02,  1.72008891e-02,  2.96802260e-02,  2.45872643e-02,\n",
       "        -3.50054279e-02,  1.40215578e-02,  3.57056479e-03, -2.18337625e-02,\n",
       "        -3.27459676e-03, -7.42057571e-03, -1.06643373e-02, -2.74281744e-02,\n",
       "        -3.60326283e-03, -1.27242561e-02,  1.61161646e-03, -8.19863286e-03,\n",
       "         4.87207323e-02, -7.77587667e-03, -1.93002801e-02, -3.90027254e-03,\n",
       "         1.76763143e-02,  3.96939106e-02, -7.08552636e-03,  4.98157516e-02,\n",
       "         2.02423595e-02,  4.43206280e-02, -1.57768596e-02,  2.18220484e-02,\n",
       "         4.30398695e-02,  1.67476702e-02,  2.71587893e-02,  3.53143513e-02,\n",
       "         7.12980563e-03, -1.57083720e-02, -3.19120591e-03,  6.08215760e-03,\n",
       "        -1.16506107e-02, -6.46562967e-03,  3.89388762e-02,  3.07612233e-02,\n",
       "         4.32931483e-02, -1.90467741e-02,  2.33990680e-02, -2.56464835e-02,\n",
       "        -8.84426571e-03,  3.67793590e-02, -2.02444699e-02,  1.12693701e-02,\n",
       "         3.85381728e-02, -1.41877737e-02,  3.15190069e-02,  4.26661447e-02,\n",
       "        -1.38675859e-02, -6.77629421e-03, -2.30524018e-02, -3.42029100e-03,\n",
       "         5.05247265e-02,  1.02016442e-02,  3.30784582e-02,  7.44002266e-03,\n",
       "         8.88163783e-03,  5.49681671e-03, -5.88537334e-03,  2.49645524e-02,\n",
       "        -4.40950952e-02,  2.41544992e-02, -9.24559031e-03, -2.81218234e-02,\n",
       "        -2.64849290e-02,  1.26624992e-02,  2.27343664e-02, -6.98833028e-03,\n",
       "         1.80240218e-02,  3.05531900e-02,  3.03791631e-02,  7.92933069e-03,\n",
       "        -1.47507507e-02,  7.73190008e-03,  9.38029028e-03,  5.80817973e-03,\n",
       "         1.11292526e-02,  1.57909619e-03, -1.53372122e-03,  2.69902106e-02,\n",
       "         1.66838355e-02,  6.14738800e-02,  1.93317570e-02, -1.45027356e-03,\n",
       "         4.82503511e-02, -1.28473984e-02, -8.71520303e-03,  3.13577615e-02,\n",
       "        -1.11046471e-02,  3.28618921e-02,  1.29759673e-03,  1.06701357e-02,\n",
       "         1.82554331e-02,  3.49452198e-02,  1.79479923e-02, -2.54205335e-03,\n",
       "         9.87347588e-02,  4.30795066e-02, -2.60084565e-03,  3.06006409e-02,\n",
       "         2.29357630e-02, -5.57605326e-02,  4.43645976e-02, -1.37514842e-03,\n",
       "         4.66882065e-03, -1.09890699e-02,  1.60625391e-02,  1.91353671e-02,\n",
       "        -3.41064632e-02,  1.97908264e-02,  5.90661447e-03,  6.85782731e-02,\n",
       "         8.07550028e-02,  5.64492121e-02,  1.66365760e-04,  1.28185633e-03,\n",
       "         3.56077729e-03,  4.15997766e-02,  7.20298756e-03,  4.28359248e-02,\n",
       "        -5.08647598e-03, -1.04536274e-02,  1.81611273e-02, -4.38723294e-03,\n",
       "         8.40747133e-02,  2.72280537e-02, -2.73021311e-02,  4.98715676e-02,\n",
       "         7.44153373e-03,  2.93694213e-02,  3.23965661e-02,  5.85119314e-02,\n",
       "         9.96590219e-03, -1.76354893e-03,  2.29098517e-02,  1.04267607e-02,\n",
       "         1.04193296e-02,  1.15912985e-02,  3.19105200e-02, -1.45548340e-02,\n",
       "         3.39811891e-02,  2.35304870e-02,  7.66624808e-02,  1.45800151e-02,\n",
       "        -2.02908907e-02,  3.66007048e-03,  6.24683406e-03,  1.59823336e-02,\n",
       "         7.58076599e-03,  1.71224959e-02,  3.62303816e-02,  2.22964641e-02,\n",
       "        -5.16966684e-03,  3.29432860e-02, -1.23597933e-02,  6.34037144e-03,\n",
       "         3.68841290e-02, -3.45323794e-03,  1.09993611e-02, -8.67988635e-03,\n",
       "        -2.99409460e-02, -4.08034734e-02,  1.62750948e-02, -3.51272672e-02,\n",
       "         1.84763595e-02,  1.73653513e-02,  3.35865712e-04, -1.17003806e-02,\n",
       "         1.69402324e-02,  4.89902347e-02,  5.03100120e-02, -1.54236332e-02,\n",
       "         3.12138419e-03,  6.25675684e-03,  6.33997694e-02,  2.37595309e-02,\n",
       "         5.00326185e-03,  7.55279288e-02, -1.24137131e-02, -1.58338706e-04,\n",
       "        -4.41687508e-03,  9.43882484e-03, -3.02045681e-02,  8.31297133e-03,\n",
       "         3.72608230e-02, -1.21186972e-02, -4.16870750e-02, -2.66624261e-02,\n",
       "         1.08015863e-02,  7.00916350e-02,  3.58944084e-03,  1.64244995e-02,\n",
       "         4.33086008e-02,  2.06068531e-02, -4.20122147e-02,  3.62375900e-02,\n",
       "         1.41655821e-02,  3.54811326e-02,  6.83303252e-02,  3.56105827e-02,\n",
       "         4.21807123e-03, -1.01990281e-02,  6.07489236e-02,  2.65131034e-02,\n",
       "        -2.14539673e-02, -9.26065724e-03,  4.02979776e-02,  1.75382458e-02,\n",
       "         2.35992088e-03,  6.98802480e-03, -1.62474383e-02,  4.95405030e-03,\n",
       "         3.61216511e-03, -2.44964585e-02, -3.22466809e-03, -1.47141246e-02,\n",
       "        -2.28235461e-02, -2.46119816e-02, -3.26703414e-02,  1.77654326e-02,\n",
       "        -1.49022285e-02,  5.49557153e-03,  3.07183322e-02, -1.90297067e-02,\n",
       "         9.43815522e-03,  1.78804398e-02,  6.01406349e-03,  1.90391205e-02,\n",
       "         3.24266553e-02,  4.30124579e-03,  6.17949143e-02,  3.53223388e-03,\n",
       "         2.55814623e-02, -1.41485333e-02,  5.02049811e-02,  5.58410585e-03,\n",
       "        -8.84239655e-03,  2.16112696e-02,  3.43300179e-02,  6.26838766e-03,\n",
       "         4.96659167e-02,  1.05452323e-02,  4.35208380e-02,  1.53963519e-02,\n",
       "        -2.77797170e-02,  4.68981713e-02, -7.74672907e-03, -1.45549504e-02,\n",
       "         1.44540304e-02,  2.00199857e-02, -8.88064783e-03,  2.98751928e-02,\n",
       "        -2.34261658e-02, -2.35889815e-02,  4.20988798e-02,  6.38480252e-03,\n",
       "         4.71395515e-02,  1.39333690e-02, -6.69862668e-04, -2.69361958e-02,\n",
       "        -4.99328040e-03, -1.40056117e-02,  1.27849716e-03,  1.23705752e-02,\n",
       "         3.35293114e-02, -2.33109053e-02,  4.13033739e-03, -4.13702913e-02,\n",
       "         2.46126205e-02, -5.76454261e-03,  2.13767346e-02, -1.12931477e-02,\n",
       "         1.26770884e-02,  3.11119705e-02,  2.72877179e-02,  1.72184184e-02,\n",
       "         1.01895900e-02,  5.39579019e-02, -2.74246614e-02, -2.40253787e-02,\n",
       "         1.85316131e-02, -6.08677184e-03,  2.50256900e-02, -1.88655723e-02,\n",
       "         2.97672860e-02,  1.09857703e-02, -9.11202573e-04,  7.33061507e-03,\n",
       "         2.21401900e-02,  2.51652449e-02,  1.28385499e-02,  1.01747839e-02,\n",
       "         1.87170915e-02,  1.00460481e-02,  4.89895009e-02,  1.95575766e-02,\n",
       "         2.80390810e-02, -9.20855720e-03,  4.83097844e-02, -1.37499273e-02,\n",
       "         9.92276445e-02,  2.78132912e-02, -1.65974963e-02,  3.05614211e-02,\n",
       "         4.93805297e-02,  1.99820735e-02, -1.01290224e-03,  5.04175723e-02,\n",
       "        -7.20117008e-03, -1.04465177e-02,  3.14176157e-02, -3.73216090e-03,\n",
       "         6.26194775e-02,  2.76269559e-02,  8.18154309e-04, -8.05452839e-03,\n",
       "         1.93753988e-02,  1.95835382e-02, -2.14989968e-02,  2.49819644e-02,\n",
       "         2.64909826e-02,  5.25853969e-02,  2.60298308e-02,  4.05217744e-02,\n",
       "         1.59905013e-02,  8.88953079e-03, -7.01020053e-03,  2.80486010e-02,\n",
       "         4.14161608e-02,  6.86831176e-02, -6.15904015e-03, -5.14614992e-02,\n",
       "         4.13982868e-02,  6.92363130e-03,  2.85715144e-02,  7.38212932e-03,\n",
       "         5.82550978e-03, -6.70510810e-04, -8.22734833e-03,  6.46974705e-03,\n",
       "        -3.24682891e-03,  1.09793441e-02, -1.69193298e-02,  2.55236812e-02],\n",
       "       dtype=float32),\n",
       " array([[-0.00345127, -0.29448515, -0.07673149, ...,  0.03039994,\n",
       "          0.04759479, -0.14001866],\n",
       "        [ 0.00403819,  0.04577785, -0.06591332, ..., -0.17489886,\n",
       "          0.09320503,  0.1030038 ],\n",
       "        [ 0.06782296,  0.05483601, -0.18915446, ...,  0.0830059 ,\n",
       "          0.09704743,  0.09891547],\n",
       "        ...,\n",
       "        [-0.16689077,  0.08086402, -0.10586202, ...,  0.06034414,\n",
       "         -0.22215647, -0.08352319],\n",
       "        [ 0.03020026,  0.13254145, -0.22923999, ...,  0.01985063,\n",
       "         -0.06633241, -0.22237596],\n",
       "        [-0.23384422, -0.00861841, -0.08290628, ...,  0.02323806,\n",
       "         -0.17330988, -0.0598339 ]], dtype=float32),\n",
       " array([-0.05000591, -0.00561423,  0.00740297, -0.02137248,  0.02076794,\n",
       "         0.0327588 , -0.00586573, -0.0073381 ,  0.01626297, -0.01220422],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows the weights that have been assigned to our model\n",
    "# getting these weights from my h5 file\n",
    "new_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.optimizers.Adam at 0x1c100c2a630>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this shows the optimizer that was used in my model\n",
    "new_model.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At predictions index 0, number is a  9\n"
     ]
    }
   ],
   "source": [
    "# predicting numbers \n",
    "predictions = new_model.predict([x_test])\n",
    "print('At predictions index 0, number is a ', np.argmax(predictions[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At predictions index 0, number is a  5\n"
     ]
    }
   ],
   "source": [
    "# predicting numbers \n",
    "predictions = new_model.predict([x_test])\n",
    "print('At predictions index 0, number is a ', np.argmax(predictions[8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At predictions index 0, number is a  2\n"
     ]
    }
   ],
   "source": [
    "# # predicting numbers \n",
    "predictions = new_model.predict([x_test])\n",
    "print('At predictions index 0, number is a ', np.argmax(predictions[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "*  helped with developing the model https://www.youtube.com/watch?v=n5a0WBIQitI\n",
    "*  helped with developing the model  youtube.com/watch?v=j-3vuBynnOE&list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN&index=2\n",
    "*  helped with saving the model https://www.youtube.com/watch?v=7n1SpeudvAE\n",
    " \n",
    "*  keras https://keras.io/\n",
    "*  tensorflow https://www.tensorflow.org/\n",
    "*  MNIST http://yann.lecun.com/exdb/mnist/\n",
    "*  Numpy https://numpy.org/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
